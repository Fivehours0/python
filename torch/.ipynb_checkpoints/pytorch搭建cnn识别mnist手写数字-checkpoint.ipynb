{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch对变量的维度要求很高，例如(30, 28, 28)30张28x28的图像，需要将其拓展为(30, 1, 28, 28),对于图片来说还包含了'厚度'，因此需要\n",
    "拓展1维，#The math of Linear: `y = xA^T + b`  因此在torch.nn.Linear中一行代表一个样例，而不是在课程中所学的一列代表一个样例。\n",
    "因此在设计变量维度时，需要注意其数学形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F   \n",
    "import torch.utils.data as Data # 小批量学习\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''加载数据'''\n",
    "path = os.path.join(os.getcwd(), 'datasets')\n",
    "def load_mnist(path, kind):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels.idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images.idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    " \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "train_images, train_labels = load_mnist(path, 'train')\n",
    "test_images, test_labels = load_mnist(path, 't10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAEICAYAAABf40E1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACqdJREFUeJzt3X+s1XUdx/HXW5CVCNyRiUZDghoo6PAPZWM0dEiOTbIrtLrzx0xza8n0j6KVZY3WJZfiFsnWXWsua2O2ZgPMDf4AdabdxhSr3bTNHyl0aylcvFwUu9xPf5yDO+Llfe/xnnvvgdfzsd3tcl/n3O+XKc/75fLlEKUUAfB1xnifAIDxRQQAc0QAMEcEAHNEADBHBABzROAUEBGzI6JExMQGfK4rImLfMB97c0Q89SGPc9LnRsSsiDgcERM+zOdGYxGBJhQRr0bEVeN9HqOllPJaKeXsUsqxep5XDcuxakCOv10xSqdpY8RfWYAx9kwpZel4n8TphCuBJhMRv5Y0S9L26le6b9XM10fEaxHxRkR8t+Y5Z0TEtyPipYh4MyJ+GxHTh3m848/rjYiuiGj94EPiZxFxKCJeiIjlNcO0iPhlRHRHxP6I+NFwLvFP/O1N9Sv8y9VzeCUirh/OuaMxiECTKaXcKOk1Sauql8w/qZmXSponabmk70fEhdWP3yHpC5KWSfqEpIOSNg/zkC9J+qykaZLWS/pNRJxfsy+W9LKkcyT9QNIjNYH5laR+SZ+WdKmkz0n66vB/tlJETJa0SdLKUsoUSUsk7U2ecmk1gv+IiLsb8X0Se6UU3prsTdKrkq6q+fFsSUXSJ2s+9mdJX66+/3dJy2u28yX9T9LEQT73FZL2JcfeK+na6vs3S/qXpDjhuDdKmiHpqKSP1mxtknbXPPepkxzj+M9noqTJknokra79XCd53hxJn1Lli9fFkrokfWe8/3ud6m9cCZxa/l3z/hFJZ1ffv0DS7yOiJyJ6VInCMVV+oaYi4qaI2Fvz3IWqfNU/bn+p/gqs+qcqVxsXSDpTUnfNczsknVvPT6iU0ifpS5K+Vv1cf4iI+Sd57MullFdKKQOllL9K+qGkNfUcDx9EBJpTvX+183VVLqdbat4+UkrZnz0pIi6Q9AtJayV9rJTSIulvkqLmYTMjovbHs1S5OnhdlSuBc2qOObWUsqDOc1cpZUcpZYUqVzAvVM9pWE894VzxIRCB5vQfVS59h+vnktqrv6gVER+PiGuH8bzJqvxC+m/1eV9R5Uqg1rmS7oiIMyPii5IulPRYKaVb0k5JGyNiavWbk3MjYlkd562ImBERn69+b+CopMOqXMUM9tiVETGj+v58SXdL2lrP8fBBRKA5/VjS96qX2d8cxuN/KmmbpJ0R0SvpT6p8Qy9VSumStFHSM6qE52JJfzzhYZ2SPiPpDUntktaUUt6sbjdJmqTK780PSvqdKl/N63GGpG+ocnVxQJVvbn79JI9dLukvEdEn6TFJj0jaUOfxcIJ4/2/3ALjhSgAwRwQAc0QAMEcEAHNEADBHBABzRAAwRwQAc0QAMEcEAHNEADBHBABzRAAwRwQAc0QAMEcEAHNj+nLNEcErmADjpJQy6OsxciUAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgLmJ430CGDsTJkxI92nTpo3q8deuXZvuZ511VrrPmzcv3W+//fZ0v++++9K9ra0t3d955510v+eee9J9/fr16T5euBIAzBEBwBwRAMwRAcAcEQDMEQHAHBEAzHGfwBiaNWtWuk+aNCndlyxZku5Lly5N95aWlnRfvXp1uo+3ffv2pfumTZvSvbW1Nd17e3vT/fnnn0/3J554It2bFVcCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgLkopYzdwSLG7mDjYNGiRem+a9eudB/tv8/f7AYGBtL9lltuSffDhw+P6Pjd3d3pfvDgwXR/8cUXR3T80VZKicE+zpUAYI4IAOaIAGCOCADmiABgjggA5ogAYI77BBpo+vTp6d7Z2Znuc+bMaeTpNNxQ59/T05PuV155Zbq/++676e5+H8VIcZ8AgEERAcAcEQDMEQHAHBEAzBEBwBwRAMzx7w400IEDB9J93bp16X7NNdek+3PPPZfuQ73u/lD27t2b7itWrEj3vr6+dF+wYEG633nnnemO0cGVAGCOCADmiABgjggA5ogAYI4IAOaIAGCO1xNoIlOnTk333t7edO/o6Ej3W2+9Nd1vuOGGdN+yZUu6o7nxegIABkUEAHNEADBHBABzRAAwRwQAc0QAMMfrCTSRt956a0TPP3To0Iief9ttt6X7ww8/nO4DAwMjOj7GB1cCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDleT+A0Mnny5HTfvn17ui9btizdV65cme47d+5Md4wvXk8AwKCIAGCOCADmiABgjggA5ogAYI4IAOa4T8DI3Llz0/3ZZ59N956ennTfvXt3uu/ZsyfdN2/enO5j+f/q6Yj7BAAMiggA5ogAYI4IAOaIAGCOCADmiABgjvsE8J7W1tZ0f/DBB9N9ypQpIzr+XXfdle4PPfRQund3d4/o+Kc77hMAMCgiAJgjAoA5IgCYIwKAOSIAmCMCgDnuE8CwLVy4MN3vv//+dF++fPmIjt/R0ZHu7e3t6b5///4RHf9Ux30CAAZFBABzRAAwRwQAc0QAMEcEAHNEADDHfQJomJaWlnRftWpVug/1egURg/4x93t27dqV7itWrEj30x33CQAYFBEAzBEBwBwRAMwRAcAcEQDMEQHAHPcJoGkcPXo03SdOnJju/f396X711Ven++OPP57upzruEwAwKCIAmCMCgDkiAJgjAoA5IgCYIwKAufwPXoEal1xySbqvWbMm3S+77LJ0H+o+gKF0dXWl+5NPPjmiz3+64koAMEcEAHNEADBHBABzRAAwRwQAc0QAMMd9AkbmzZuX7mvXrk336667Lt3PO++8us+pHseOHUv37u7udB8YGGjk6Zw2uBIAzBEBwBwRAMwRAcAcEQDMEQHAHBEAzHGfwClkqD+Hb2trS/eh7gOYPXt2vafUUHv27En39vb2dN+2bVsjT8cGVwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOe4TGEMzZsxI94suuijdH3jggXSfP39+3efUSJ2dnel+7733pvvWrVvTndcDGB1cCQDmiABgjggA5ogAYI4IAOaIAGCOCADmuE+gDtOnT0/3jo6OdF+0aFG6z5kzp+5zaqSnn3463Tdu3JjuO3bsSPe333677nPC6ONKADBHBABzRAAwRwQAc0QAMEcEAHNEADBndZ/A4sWL033dunXpfvnll6f7zJkz6z6nRjpy5Ei6b9q0Kd03bNiQ7n19fXWfE5ofVwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOav7BFpbW0e0j1RXV1e6P/roo+ne39+f7kP9ff+enp50hyeuBABzRAAwRwQAc0QAMEcEAHNEADBHBABzUUoZu4NFjN3BALxPKSUG+zhXAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgDkiAJgjAoA5IgCYIwKAOSIAmCMCgLkx/XcHADQfrgQAc0QAMEcEAHNEADBHBABzRAAw93/4eKjLPBxHewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hyper parameters\n",
    "EPOCH = 1\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "plt.figure()\n",
    "plt.matshow(train_images[0].reshape(28, 28), cmap='gray')\n",
    "plt.title('the label is %i'%train_labels[0],)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "'''小批量学习'''\n",
    "train_images = train_images.reshape(60000, 28, 28) / 255 \n",
    "train_images = torch.unsqueeze(torch.Tensor(train_images), dim=1)\n",
    "train_labels = torch.Tensor(train_labels)\n",
    "train_labels = train_labels.type(torch.long)\n",
    "tensor_data = Data.TensorDataset(train_images, train_labels)\n",
    "loader = Data.DataLoader(tensor_data, batch_size=BATCH_SIZE, shuffle=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c870c8a3106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mloss_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LR' is not defined"
     ]
    }
   ],
   "source": [
    "'''卷积网络搭建'''\n",
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # ->(1, 28, 28)\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1, # 图片的'厚度'为1\n",
    "                out_channels=16, # number of filters\n",
    "                kernel_size=5,# 核大小\n",
    "                stride=1,# 步长\n",
    "                padding=2\n",
    "            ),  #2d表示图片的维度是2维的\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )# ->(16, 14, 14)\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        ) # ->(32, 7, 7)\n",
    "        self.out = torch.nn.Linear(32*7*7, 10)# 一个样例输出10个，一个样例一行\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x) # ->(batch, 37, 7, 7)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "cnn = CNN()    \n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "loss_arr = []\n",
    "for step, (x_train_batch, y_train_batch) in enumerate(loader):\n",
    "    output = cnn(x_train_batch)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_func(output, y_train_batch)\n",
    "    loss_arr.append(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "torch.save(cnn.state_dict(), 'cnn-params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # ->(1, 28, 28)\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1, # 图片的'厚度'为1\n",
    "                out_channels=16, # number of filters\n",
    "                kernel_size=5,# 核大小\n",
    "                stride=1,# 步长\n",
    "                padding=2\n",
    "            ),  #2d表示图片的维度是2维的\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        )# ->(16, 14, 14)\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2)\n",
    "        ) # ->(32, 7, 7)\n",
    "        self.out = torch.nn.Linear(32*7*7, 10)# 一个样例输出10个，一个样例一行\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x) # ->(batch, 37, 7, 7)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        output = self.out(x)\n",
    "        return output\n",
    "cnn = CNN()\n",
    "cnn.load_state_dict(torch.load('cnn-params.pkl'))\n",
    "y_prediction = cnn(train_images[:20000])# 同时预测60000张会爆内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft = torch.nn.Softmax(dim=1)\n",
    "y_prediction = soft(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9815)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre_lable = y_prediction.max(dim=1)[1]\n",
    "torch.sum(y_pre_lable==train_labels[:20000]).type(torch.float) / 20000 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
