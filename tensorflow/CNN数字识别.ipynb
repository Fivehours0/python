{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    全连接层初始化的w参数过大，导致输入softmax数值非常大(百位级别,最大的数为700多，最小的数为-800多)，这将会导致softmax输出的数组中基本上都为0，只有700多的那个数为1，因此在add_layer函数中初始化w的部分除以100，缩小了w，使得输出结果正常。\n",
    "    在调试过程中遇到代码出现问题，可以试试在会话中打印数据，观察数据的准确性来发现问题的所在，例如可以把下面打印代码的prediction改为xs观察输入数据的正确型。也可以加.shape观察数据形状\n",
    "    print(sess.run(prediction, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5}))\n",
    "    在全连接层中，对于x，样本为数为行，属性数为列。y = xw + b。因为tensorflow对张量的定义为(样本数，长，宽，通道数(rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ATTRIBUTE = 784\n",
    "N_OUTPUT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join('/home/secret', 'datasets')\n",
    "def load_mnist(path, kind):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels.idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images.idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    " \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "train_images, train_labels = load_mnist(path, 'train')\n",
    "test_images, test_labels = load_mnist(path, 't10k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对图片顺序进行打乱\n",
    "shuffle_train_index = np.random.permutation(60000)\n",
    "x_train_shuffle, y_train_shuffle = train_images[shuffle_train_index], train_labels[shuffle_train_index]\n",
    "shuffle_test_index = np.random.permutation(10000)\n",
    "x_test_shuffle, y_test_shuffle = test_images[shuffle_test_index], test_labels[shuffle_test_index]\n",
    "\n",
    "x_test_shuffle = x_test_shuffle.astype(np.float32) / 255\n",
    "y_test_shuffle = y_test_shuffle.astype(np.float32)\n",
    "x_train_shuffle = x_train_shuffle.astype(np.float32) / 255\n",
    "y_train_shuffle = y_train_shuffle.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''对label进行onehot编码'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OneHot = OneHotEncoder(sparse=False, categories='auto')\n",
    "y_train_enconded = OneHot.fit_transform(y_train_shuffle.reshape(-1, 1))\n",
    "y_test_enconded = OneHot.fit_transform(y_test_shuffle.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全连接层\n",
    "#对于x，样本为数为行，属性数为列。\n",
    "# y = xw + b\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    weight = tf.Variable(tf.random_normal([in_size, out_size]) / 100)\n",
    "    biases = tf.Variable(tf.zeros([1, out_size]))\n",
    "    xw_plus_b = tf.matmul(inputs, weight) + biases\n",
    "    if activation_function == None:\n",
    "        return xw_plus_b\n",
    "    else:\n",
    "        if activation_function == tf.nn.softmax:\n",
    "            return activation_function(xw_plus_b, axis=1)\n",
    "        else:\n",
    "            return activation_function(xw_plus_b)\n",
    "    \n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 0.5})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre, 1), tf.argmax(v_ys, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 0.5})\n",
    "    return result\n",
    "\n",
    "def weight_variable(shape):\n",
    "    # 截断正态分布\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1) # 方差越小，数据越集中\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):       \n",
    "    #ksize为核大小2x2x1\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#placeholder\n",
    "xs = tf.placeholder(tf.float32, [None, N_ATTRIBUTE])\n",
    "ys = tf.placeholder(tf.float32, [None, N_OUTPUT])\n",
    "keep_prob = tf.placeholder(tf.float32) # 随机失活dropout的参数\n",
    "x_image = tf.reshape(xs, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conv1 layer ## \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])# 5x5x1  32个 因为对图片进行了same padding，所有图片的长宽不变\n",
    "b_conv1 = bias_variable([32])# 一个卷积核一个偏置项\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)# output size 28x28x32\n",
    "h_pool1 = max_pool_2x2(h_conv1) # output size 14x14x32\n",
    "\n",
    "## conv2 layer ## \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])# 5x5x32  64个 因为对图片进行了same padding，所有图片的长宽不变\n",
    "b_conv2 = bias_variable([64])# 一个卷积核一个偏置项\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)# output size 14x14x64\n",
    "h_pool2 = max_pool_2x2(h_conv2) # output size 7x7x64\n",
    "\n",
    "## fullc1 layer ##\n",
    "#[n_samples, 7, 7, 64] -> [n_samples, 7*7*64]\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = add_layer(h_pool2_flat, 7*7*64, 1024, activation_function=tf.nn.relu)\n",
    "#h_fc1_dropout = tf.nn.dropout(h_fc1, rate=keep_prob)\n",
    "\n",
    "## fullc2 layer ##\n",
    "prediction = add_layer(h_fc1, 1024, 10, activation_function=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_entropy_array = [] # 保存cross_entropy(损失)用以绘制图像\n",
    "# 先sum在mean时除数为sum之后的个数， 如果只有mean则为所有数的个数\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys*tf.log(prediction), reduction_indices=1))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        batch_xs = x_train_shuffle[60*i: 60*(i+1), :]\n",
    "        batch_ys = y_train_enconded[60*i: 60*(i+1), :]\n",
    "        #print(sess.run(prediction, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5}))\n",
    "        sess.run(train_step, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5})\n",
    "        cross_entropy_array.append(sess.run(cross_entropy, feed_dict={xs: batch_xs, ys: batch_ys, keep_prob: 0.5}))\n",
    "        if i % 50 == 0:\n",
    "            print(compute_accuracy(x_test_shuffle, y_test_enconded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''可视化cross_entropy'''\n",
    "cross_entropy_array = np.array(cross_entropy_array)\n",
    "plot_x = np.arange(1000)\n",
    "plt.figure()\n",
    "plt.plot(plot_x, cross_entropy_array, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = (np.array([1.79982986e+02, 6.37160217e2, -8.27999634e+02, -3.87886353e+01,\n",
    "  -4.91421814e+01, -5.50225830e+01,  7.63858093e+02, -8.44356995e+01,\n",
    "   8.87969849e+02, 8.05480835e+02]) ).reshape([1, -1])\n",
    "import torch\n",
    "arr1 = torch.from_numpy(arr1)\n",
    "soft = torch.nn.Softmax(dim=1)\n",
    "print(soft(arr1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
