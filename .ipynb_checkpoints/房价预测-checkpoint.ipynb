{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'最重要的是收入中位数与房价中位数的图，这张图说明了几点。首先,相关性非常高;\\n可以清晰地看到向上的趋势,并且数据点不是非常分散。第二,我们之前看到的最高价,\\n清晰地呈现为一条位于 500000 美元的水平线。这张图也呈现了一些不是那么明显的\\n直线:一条位于 450000 美元的直线,一条位于 350000 美元的直线,一条在 280000 美元\\n的线,和一些更靠下的线,这些数据的巧合需要在给算法提供数据之前将其去除'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from six.moves import urllib\n",
    "Download_Root = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "House_Path = \"datasets/housing\"\n",
    "House_Url = Download_Root + House_Path + '/housing.tgz'\n",
    "\n",
    "'''获取数据'''\n",
    "def fetch_housing_data(housing_path=House_Path, housing_url=House_Url):  \n",
    "    if(os.path.isdir(housing_path)==False):    \n",
    "        os.makedirs(housing_path)  #创建目录\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')  #合并数据\n",
    "    if(os.path.exists(tgz_path)==False):\n",
    "        urllib.request.urlretrieve(housing_url, tgz_path)  #网上下载housing.tgz数据包\n",
    "        housing_tgz = tarfile.open(tgz_path)  #打开压缩未见\n",
    "        housing_tgz.extractall(housing_path)  #将压缩文件进行提取\n",
    "        housing_tgz.close()    #提取完成后关闭\n",
    "'''加载csv格式的房屋数据'''\n",
    "def load_housing_data(housing_path=House_Path):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "'''hash采样'''\n",
    "'''利用hash序列选取数据达到每次选取的测试集不变'''\n",
    "'''这里以index作为id，做好用数据中不变的特征经过适当处理作为id，例如经纬度乘以10'''\n",
    "def test_set_check(identifier, test_ratio, func):\n",
    "    return func(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
    "    #这里若不使用匿名函数，hashlib只返回index列第一个数的结果\n",
    "def split_train_test_by_id(data, test_ratio, id_column, func=hashlib.md5):\n",
    "    ids = data[id_column]\n",
    "    #in_test_set 为布尔量\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, func)) \n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "#housing_with_id = df.reset_index()  #添加了index这一column作为数据的id\n",
    "#train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\n",
    "\n",
    "'''sklearn调用函数进行采样'''\n",
    "#train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "'''分层采样'''\n",
    "'''例如需要在城市中抽1000人做调查，不可以随机抽取'''\n",
    "'''若该城市男女比例为6:4,则应该选600男400女，这样的采样数据更能代表整个数据集'''\n",
    "#df['income_cat'] = np.ceil(df.median_income / 1.5) #创建新列，缩小数据值便于分组\n",
    "##不满足条件的值用5替代，inplace=True表示原地替换\n",
    "#df['income_cat'].where(df.income_cat < 5, other=5, inplace=True) \n",
    "##分层采样函数 n_splits表示进行洗牌的次数，可以洗三次，取第三次分层结果\n",
    "#split_data = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "#for train_index,test_index in split_data.split(df, df.income_cat):  #输入x和y\n",
    "#    strat_train_set = df.loc[train_index]\n",
    "#    strat_test_set = df.loc[test_index]\n",
    "\n",
    "#1获取网络上的数据\n",
    "#2加载数据至程序中\n",
    "#3采样测试集(hash采样，sklearn调用函数采样，分层采样）\n",
    "#4数据探索，可视化，发现规律\n",
    "#5为机器学习算法准备数据\n",
    "#6选择并训练模型\n",
    "#7模型微调\n",
    "#8分析最佳模型和他们的误差\n",
    "#1\n",
    "fetch_housing_data()    \n",
    "#2\n",
    "df = load_housing_data() \n",
    "#df.info()  #返回df的信息\n",
    "#df['ocean_proximity'].value_counts()  #该columns的值为类别，可以用value_counts查看有那些类别和类别的数量\n",
    "#df.describe()  #输出每个column的mean,std, max, min等参数\n",
    "#df.hist(bins=50, figsize=(20, 15))  #生成柱状图\n",
    "#plt.show()\n",
    "#3\n",
    "df['income_cat'] = np.ceil(df.median_income / 1.5) #创建新列，缩小数据值便于分组 ceil向上取整\n",
    "#不满足条件的值用5替代，inplace=True表示原地替换\n",
    "df['income_cat'].where(df.income_cat < 5, other=5, inplace=True) \n",
    "split_data = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index,test_index in split_data.split(df, df.income_cat):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "for values in [strat_train_set, strat_test_set]:\n",
    "    values.drop('income_cat', axis=1, inplace=True)\n",
    "#4\n",
    "'''如果训练集很大，可以采样一个探测集用以发现数据的规律(探索）'''\n",
    "rep_train = strat_train_set.copy() #创建一个副本，以免损伤训练集\n",
    "#可视化\n",
    "#类型为散点图 x为rep_train中经度值 y为纬度值 透明度0.1 形状以population设置 \n",
    "#图例为population 颜色以房屋价格中位数设置 cmap为jet型 colorbar可以改为False就知道是干嘛的了\n",
    "plt.figure()\n",
    "rep_train.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1, s=rep_train.population/100,\n",
    "               label='population', c='median_house_value', cmap=plt.cm.jet, colorbar=True)\n",
    "plt.show()\n",
    "# 发现规律\n",
    "#相关系数的范围是-1到1。当接近1时,意味强正相关;当相关系数接近-1时,意味强负相关;相关系数接近0,意味没有线性相关性\n",
    "#相关系数只测量线性关系，可能完全忽视非线性关系\n",
    "corr_matrix = rep_train.corr()  # 输出所有特征之间的相关系数矩阵\n",
    "corr_matrix.median_house_value.sort_values(ascending=False)  # 对房价中位数与其他特征的相关系数进行排序输出\n",
    "# pd中有scatter_matrix函数能画出每个数值属性对每个其它数值属性的图\n",
    "# from pandas.plotting import scatter_matrix\n",
    "# attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "# scatter_matrix(rep_train[attributes], figsize=(12, 8))\n",
    "# plt.show()\n",
    "'''最重要的是收入中位数与房价中位数的图，这张图说明了几点。首先,相关性非常高;\n",
    "可以清晰地看到向上的趋势,并且数据点不是非常分散。第二,我们之前看到的最高价,\n",
    "清晰地呈现为一条位于 500000 美元的水平线。这张图也呈现了一些不是那么明显的\n",
    "直线:一条位于 450000 美元的直线,一条位于 350000 美元的直线,一条在 280000 美元\n",
    "的线,和一些更靠下的线,这些数据的巧合需要在给算法提供数据之前将其去除'''\n",
    "# rep_train.plot(kind=\"scatter\", x=\"median_income\",y=\"median_house_value\", alpha=0.5)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#属性组合测试\n",
    "rep_train['rooms_per_household'] = rep_train['total_rooms'] / rep_train['households']# 每户家庭拥有的房间数\n",
    "rep_train['bedrooms_pre_room'] = rep_train['total_bedrooms'] / rep_train['total_rooms']# 卧室数目占总房间数目\n",
    "rep_train['population_per_household'] = rep_train['population'] / rep_train['households']# 每户的人口数\n",
    "corr_matrix = rep_train.corr() #相关系数矩阵，正相关以及负相关都具有信息\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False) #排序，降序\n",
    "'''这一步的数据探索不必非常完备,此处的目的是有一个正确的开始,快速发现规律,以得到\n",
    "一个合理的原型(可以想象成函数，哪个是x，函数系数又是多少)。但是这是一个交互过程:一旦你得到了一个原型,并运行起来,你就可以\n",
    "分析它的输出,进而发现更多的规律,然后再回到数据探索这步。'''\n",
    "#5\n",
    "#为机器学习算法准备数据\n",
    "rep_train = strat_train_set.drop('median_house_value', axis=1)\n",
    "train_labels = strat_train_set['median_house_value'].copy()  #若不加copy()则两变量指向同一个区域，修改一个则另一个也被修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计数据中na的数目，value_counts只支持Series, 发现total_bedrooms中有158个na\n",
    "# for columns in rep_train.columns:\n",
    "#     print(columns, rep_train[columns].isna().value_counts())、\n",
    "\n",
    "#数据清洗\n",
    "#处理数据缺失的问题：1.去掉对应的街区;2.去掉整个属性;3.进行赋值(0、平均值、中位数等等)\n",
    "#1.rep_train.dropna(subset=['total_bedrooms'])  #subset需要进行处理的列\n",
    "#2.rep_train.drop(['total_bedrooms'], axis=1)\n",
    "#3.median = rep_train['total_bedrooms'].median()\n",
    "  #rep_train['total_bedrooms'].fillna(median)\n",
    "#sklearn中提供了方便的类来处理缺失值\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "rep_train_num = rep_train.drop('ocean_proximity', axis=1) # 因为ocean_proximity列不是数据，无法计算中位数，所以去掉\n",
    "imputer.fit(rep_train_num)\n",
    "# imputer.statistics_# 查看每列的median。rep_train_num.median().values同样的效果\n",
    "X = imputer.transform(rep_train_num)# 对缺失数据进行填充,返回值为numpy数组\n",
    "housing_tr = pd.DataFrame(X, columns=rep_train_num.columns)\n",
    "\n",
    "#处理文本和类别属性\n",
    "#sklearn为将文本转换为数据提供了一个转换器\n",
    "\n",
    "#LabelEncoder适用与Label(只有一列的文本特征)(n_sample,)\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder = LabelEncoder()\n",
    "# housing_cat = rep_train['ocean_proximity']\n",
    "# housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "# encoder.classes_# 查看映射表\n",
    "\n",
    "#使用LabelBinarizer可以将文本转换为数字，将数字转换为独热码这两步结合\n",
    "\n",
    "#对于文本属性，使用factorize较优\n",
    "housing_cat = rep_train['ocean_proximity']\n",
    "housing_cat_encoded, housing_categories = housing_cat.factorize() # 返回类型编码， 编码所对应的类型集合\n",
    "#factorize按类型返回的是[0, 1, 2, 3, 4](有5个类别)，这会在训练时使得算法认为0和1比0和5更相似，要解决这个问题，采用独热编码\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "housing_cat_1hot = one_hot_encoder.fit_transform(housing_cat_encoded.reshape(-1, 1)) # 输出结果是SCIPY稀疏矩阵，存储大量的0浪费空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'除了个别情况,当输入的数值属性量度不同时,机器学习算法的性能都不会好。通常情况下我们不需要对目标值进行缩放。有两种常见的方法，\\n线性函数归一化，即将数据压缩至0-1(MinMaxScaler()函数). 2.标准化0均值单位方差(StandardScaler)。与归一化不同,标准化不会\\n限定值到某个特定的范围,这对某些算法可能构成问题(比如,神经网络常需要输入值得范围是 0 到 1)。但是,标准化受到异常值的影响很小。\\n例如,假设一个街区的收入中位数由于某种错误变成了100,归一化会将其它范围是 0 到 15 的值变为 0-0.15,但是标准化不会受什么影响。'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义转换器\n",
    "'''转换器有一个超参数add_bedrooms_per_room.这个超参数可以让你方便地发现添加了这个属性是否对机器学习算法有帮助。\n",
    "你可以为每个不能完全确保的数据准备步骤添加一个超参数。数据准备步骤越自动化,可以自动化的操作组合就越多,越容易发现更好用的组合(并能节省大量\n",
    "时间)。'''\n",
    "from sklearn.base import TransformerMixin #自定义转换器继承TransformerMixin可以拥有fit_transform功能\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "class CombinedAttributes(TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room;\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "attr_adder = CombinedAttributes(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(df.values)\n",
    "#特征缩放\n",
    "'''除了个别情况,当输入的数值属性量度不同时,机器学习算法的性能都不会好。通常情况下我们不需要对目标值进行缩放。有两种常见的方法，\n",
    "线性函数归一化，即将数据压缩至0-1(MinMaxScaler()函数). 2.标准化0均值单位方差(StandardScaler)。与归一化不同,标准化不会\n",
    "限定值到某个特定的范围,这对某些算法可能构成问题(比如,神经网络常需要输入值得范围是 0 到 1)。但是,标准化受到异常值的影响很小。\n",
    "例如,假设一个街区的收入中位数由于某种错误变成了100,归一化会将其它范围是 0 到 15 的值变为 0-0.15,但是标准化不会受什么影响。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<1H OCEAN' 'INLAND' 'ISLAND' 'NEAR BAY' 'NEAR OCEAN']\n"
     ]
    }
   ],
   "source": [
    "#转换流水线\n",
    "#Scikit-Learn 没有工具来处理DataFrame,因此我们需要写一个简单的自定义转换器来做这项工作:\n",
    "class DataFrameSelector(TransformerMixin):\n",
    "    def __init__(self, attribute_name):\n",
    "        self.attribute_name = attribute_name;\n",
    "    def fit(self, X, y=None):\n",
    "        return self;\n",
    "    def transform(self, X, y=None):\n",
    "        if(X[self.attribute_name].shape[1]==1):\n",
    "            return X[self.attribute_name].values.reshape(-1,)\n",
    "        else:\n",
    "            return X[self.attribute_name].values# .values返回的是numpy.array\n",
    "'''The pipeline is assuming LabelEncoder's fit_transform method is defined to take three positional arguments:\n",
    "def fit_transform(self, x, y)\n",
    "while it is defined to take only two:\n",
    "def fit_transform(self, x)'''\n",
    "#因此自己根据LabelEncoder编写自定义转换器\n",
    "class CharToInt(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = LabelEncoder()\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        encoder = LabelEncoder()\n",
    "        data_encoded = self.encoder.fit_transform(X)\n",
    "        print(self.encoder.classes_)\n",
    "        return data_encoded.reshape((-1,1))\n",
    "        \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder #这里为了展示流水线而将标签编码应用与属性编码(此处只转换一列，故可以应用)\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "num_attribts = list(rep_train_num.columns)\n",
    "cat_attribts = ['ocean_proximity']\n",
    "#数字数据处理流水线\n",
    "num_pipeline = Pipeline(\n",
    "    [('select', DataFrameSelector(num_attribts)),\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('attribs_adder', CombinedAttributes()),\n",
    "    ('std_scaler', StandardScaler())\n",
    "    ])\n",
    "#文本数据处理流水线\n",
    "cat_pipeline = Pipeline([\n",
    "    ('select', DataFrameSelector(cat_attribts)),\n",
    "    ('char_to_int', CharToInt()),\n",
    "    ('1hot', OneHotEncoder(categories='auto',sparse=False))\n",
    "    ])\n",
    "#将两个流水线并行执行，等待输出，最后将结果合并起来，和np.c_[]合并方式一样\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline),\n",
    "    ])\n",
    "housing_prepared = full_pipeline.fit_transform(rep_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'不要在调节超参数上花费太多时间。目标是列出一个可能模型的列表(两到五个)。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6\n",
    "#选择并训练模型\n",
    "#训练\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, train_labels)\n",
    "\n",
    "#查看一下前5个数据的预测值\n",
    "# some_data = housing_prepared[:5]\n",
    "# some_label = train_labels[:5]\n",
    "# some_data_predict = lin_reg.predict(some_data)\n",
    "# print(some_label, '\\n', some_data_predict)\n",
    "\n",
    "#查看训练集的RMSE \n",
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predict = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(train_labels, housing_predict)\n",
    "lin_rmse = np.sqrt(lin_mse)  # 为68628.19819848923\n",
    "'''大多数街区的median_housing_values位于 120000到 265000 美元之间,因此预测误差 68628 美元不能让人满意。这是一个模型欠拟合训练数\n",
    "据的例子。当这种情况发生时,意味着特征没有提供足够多的信息来做出一个好的预测,或者模型并不强大。就像前一章看到的,修复欠拟合的主要方法是\n",
    "选择一个更强大的模型,给训练算法提供更好的特征,或去掉模型上的限制。这个模型还没有正则化,所以排除了最后一个选项。\n",
    "你可以尝试添加更多特征(比如,人口的对数值),但是首先让我们尝试一个更为复杂的模型,看看效果。'''\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, train_labels)\n",
    "housing_predictions = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "tree_rmse = np.sqrt(tree_mse) # 为0.0，很有可能是模型发生了过拟合，因此使用交叉验证对模型评估\n",
    "# 交叉验证\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_prepared, train_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "# print('scores:', rmse_scores, '\\n', 'mean:', rmse_scores.mean(), '\\n', 'std:', rmse_scores.std())\n",
    "# 可以看出，模型的性能比线性回归的还要差，可以判断为过拟合\n",
    "\n",
    "# 随机森林\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor(n_estimators=10)\n",
    "forest_reg.fit(housing_prepared, train_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "\n",
    "\n",
    "'''不要在调节超参数上花费太多时间。目标是列出一个可能模型的列表(两到五个)。'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=8,\n",
       "       param_grid=[{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7\n",
    "#模型微调\n",
    "'''假设你现在有了一个列表,列表里有几个有希望的模型。你现在需要对它们进行微调(超参数)'''\n",
    "#网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(estimator=forest_reg, param_grid=param_grid, cv=5, n_jobs=8)\n",
    "grid_search.fit(housing_prepared, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855238804209186 {'max_features': 2, 'n_estimators': 3}\n",
      "0.7663731242472611 {'max_features': 2, 'n_estimators': 10}\n",
      "0.7904943499115977 {'max_features': 2, 'n_estimators': 30}\n",
      "0.7254525028665252 {'max_features': 4, 'n_estimators': 3}\n",
      "0.7904481052115742 {'max_features': 4, 'n_estimators': 10}\n",
      "0.8079573699843191 {'max_features': 4, 'n_estimators': 30}\n",
      "0.7467826890484041 {'max_features': 6, 'n_estimators': 3}\n",
      "0.7977191191513435 {'max_features': 6, 'n_estimators': 10}\n",
      "0.8129806059675941 {'max_features': 6, 'n_estimators': 30}\n",
      "0.7459228956279965 {'max_features': 8, 'n_estimators': 3}\n",
      "0.7989559752725026 {'max_features': 8, 'n_estimators': 10}\n",
      "0.8131401707454239 {'max_features': 8, 'n_estimators': 30}\n",
      "0.7065517866344345 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "0.7786930523434362 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "0.7325229767587783 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "0.7948854064312149 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "0.7413870470664351 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "0.8003698350900598 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "#使用网格搜索后的最优模型进行预测\n",
    "forest_reg = grid_search.best_estimator_\n",
    "forest_reg.fit(housing_prepared, train_labels)\n",
    "housing_predictions = forest_reg.predict(housing_prepared)\n",
    "forest_mse = mean_squared_error(train_labels, housing_predictions)\n",
    "forest_rmse = np.sqrt(forest_mse) \n",
    "forest_rmse\n",
    "\n",
    "cvres = grid_search.cv_results_ #显示每一对超参数的得分和参数\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(mean_score, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3780032658537155, 'median_income'),\n",
       " (0.16040106764518214, 'INLAND'),\n",
       " (0.11268142582772255, 'pop_per_hhold'),\n",
       " (0.06740474363119267, 'bedrooms_per_room'),\n",
       " (0.06412963967564875, 'longitude'),\n",
       " (0.0580162746589019, 'latitude'),\n",
       " (0.04339447072841226, 'housing_median_age'),\n",
       " (0.041112823097615325, 'rooms_per_hhold'),\n",
       " (0.015594366756121589, 'total_rooms'),\n",
       " (0.015431603778023571, 'population'),\n",
       " (0.015086311677596064, 'total_bedrooms'),\n",
       " (0.014671403478780825, 'households'),\n",
       " (0.009488025581239911, '<1H OCEAN'),\n",
       " (0.0028491515888301533, 'NEAR OCEAN'),\n",
       " (0.001643918414461886, 'NEAR BAY'),\n",
       " (9.150760655479202e-05, 'ISLAND')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#随机搜索\n",
    "'''当超参数的搜索空间很大时,随机搜索RandomizedSearchCV，优点：如果你让随机搜索运行,比如 1000 次,它会探索每个超参数的 1000 个不同的值\n",
    "(而不是像网格搜索那样,只搜索每个超参数的几个值)。你可以方便地通过设定搜索次数,控制超参数搜索的计算量。'''\n",
    "#8\n",
    "#分析最佳模型和他们的误差\n",
    "#分析最佳模型，指出每个属性对于做出预测值的重要性\n",
    "feature_import = grid_search.best_estimator_.feature_importances_\n",
    "extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n",
    "cat_one_hot_attribs = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "attributes = num_attribts + extra_attribs + cat_one_hot_attribs\n",
    "sorted(zip(feature_import, attributes), reverse=True)\n",
    "'''有了这个信息,你就可以丢弃一些不那么重要的特征(比如,显然只要一个ocean_proximity的类型(INLAND)就够了,所以可以丢弃掉其它的),你还应该\n",
    "看一下系统犯的误差,搞清为什么会有些误差,以及如何改正问题(添加更多的特征,或相反,去掉没有什么信息的特征,清洗异常值等等)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早停\n",
    "# from sklearn.base import clone\n",
    "# if val_error < minimum_val_error:\n",
    "#     minimum_val_error = val_error\n",
    "#     best_epoch = epoch\n",
    "#     best_model = clone(sgd_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<1H OCEAN' 'INLAND' 'ISLAND' 'NEAR BAY' 'NEAR OCEAN']\n"
     ]
    }
   ],
   "source": [
    "#用测试集评估系统\n",
    "final_model = grid_search.best_estimator_\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test) #这里使用的是transform而不是fit_transform,因为之前fit过了\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "'''在测试集上的性能较差时，一定不要去调节超参数，使得测试集的效果变好，这样的提升不能推广到新数据上'''\n",
    "# => evaluates to 48,209.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#def f(x):\n",
    "#    return pd.Series([x.max(), x.min()], index=['max', 'min'])\n",
    "#f = lambda x: x.max() - x.min()\n",
    "f = lambda x: '%.2f' % x\n",
    "df = pd.DataFrame(np.arange(12).reshape(3, 4))\n",
    "print(df)\n",
    "deal_df = df.applymap(f)\n",
    "print(deal_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
